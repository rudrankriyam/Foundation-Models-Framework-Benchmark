import Foundation

// MARK: - Metrics & Reporting

/// Performance metrics collected during a benchmark run.
///
/// `BenchmarkMetrics` captures timing information, token counts, and calculated
/// throughput metrics for a single benchmark execution. All timing values are
/// measured in seconds, and token counts are estimates based on character counts.
///
/// ## Example
///
/// ```swift
/// let metrics = BenchmarkMetrics(
///     start: startDate,
///     end: endDate,
///     timeToFirstToken: 0.5,
///     promptTokenEstimate: 125,
///     responseTokenEstimate: 1069
/// )
/// print("Tokens/sec: \(metrics.tokensPerSecond ?? 0)")
/// ```
public struct BenchmarkMetrics: Codable, Sendable {
    /// The start time of the benchmark run.
    public let start: Date

    /// The end time of the benchmark run.
    public let end: Date

    /// The total duration of the benchmark run in seconds.
    public let duration: TimeInterval

    /// The time elapsed before the first token was received, or `nil` if not measured.
    public let timeToFirstToken: TimeInterval?

    /// The estimated number of tokens in the prompt.
    public let promptTokenEstimate: Int

    /// The estimated number of tokens in the response.
    public let responseTokenEstimate: Int

    /// The total estimated number of tokens (prompt + response).
    public let totalTokenEstimate: Int

    /// The calculated tokens per second throughput, or `nil` if duration is zero.
    public let tokensPerSecond: Double?

    /// Creates a new benchmark metrics instance.
    ///
    /// The duration and tokens per second are automatically calculated from the
    /// provided parameters.
    ///
    /// - Parameters:
    ///   - start: The start time of the benchmark.
    ///   - end: The end time of the benchmark.
    ///   - timeToFirstToken: The time elapsed before the first token was received,
    ///     or `nil` if not measured.
    ///   - promptTokenEstimate: The estimated number of tokens in the prompt.
    ///   - responseTokenEstimate: The estimated number of tokens in the response.
    public init(
        start: Date,
        end: Date,
        timeToFirstToken: TimeInterval?,
        promptTokenEstimate: Int,
        responseTokenEstimate: Int
    ) {
        self.start = start
        self.end = end
        self.duration = end.timeIntervalSince(start)
        self.timeToFirstToken = timeToFirstToken
        self.promptTokenEstimate = promptTokenEstimate
        self.responseTokenEstimate = responseTokenEstimate
        self.totalTokenEstimate = promptTokenEstimate + responseTokenEstimate
        if duration > 0 {
            self.tokensPerSecond = Double(totalTokenEstimate) / duration
        } else {
            self.tokensPerSecond = nil
        }
    }
}

/// The complete result of a benchmark run.
///
/// `BenchmarkResult` contains all information collected during a benchmark
/// execution, including the prompt used, performance metrics, environment
/// information, and the complete response text from the model.
///
/// ## Example
///
/// ```swift
/// let result = try await runner.run()
/// print("Device: \(result.environment.deviceName)")
/// print("Duration: \(result.metrics.duration)s")
/// print("Response: \(result.responseText)")
/// ```
public struct BenchmarkResult: Codable, Sendable {
    /// The prompt that was used for this benchmark run.
    public let prompt: BenchmarkPrompt

    /// The performance metrics collected during the benchmark.
    public let metrics: BenchmarkMetrics

    /// The environment snapshot captured at the time of the benchmark.
    public let environment: EnvironmentSnapshot

    /// The complete response text generated by the model.
    public let responseText: String
}

/// A report generator for benchmark results.
///
/// `BenchmarkReport` provides methods to serialize benchmark results into
/// various formats, including JSON and Markdown. Use this to export or share
/// benchmark data.
///
/// ## Example
///
/// ```swift
/// let result = try await runner.run()
/// let report = BenchmarkReport(result: result)
/// let json = try report.json(prettyPrinted: true)
/// let markdown = report.markdownSummary()
/// ```
public struct BenchmarkReport: Codable, Sendable {
    /// The benchmark result to generate a report from.
    public let result: BenchmarkResult

    /// Creates a new benchmark report from a benchmark result.
    ///
    /// - Parameter result: The benchmark result to report on.
    public init(result: BenchmarkResult) {
        self.result = result
    }

    /// Serializes the benchmark result as JSON.
    ///
    /// - Parameter prettyPrinted: Whether to format the JSON with indentation
    ///   and line breaks. Defaults to `true`.
    /// - Returns: A JSON string representation of the benchmark result.
    /// - Throws: An error if encoding fails.
    public func json(prettyPrinted: Bool = true) throws -> String {
        let encoder = JSONEncoder()
        encoder.outputFormatting = prettyPrinted ? [.prettyPrinted, .sortedKeys] : []
        encoder.dateEncodingStrategy = .iso8601
        let data = try encoder.encode(result)
        guard let jsonString = String(data: data, encoding: .utf8) else {
            throw CocoaError(.coderInvalidValue)
        }
        return jsonString
    }

    /// Generates a Markdown-formatted summary of the benchmark result.
    ///
    /// The summary includes timestamp, device information, metrics, and the
    /// complete response text formatted for easy reading.
    ///
    /// - Returns: A Markdown string containing the benchmark summary.
    public func markdownSummary() -> String {
        """
        # Foundation Models Benchmark

        **Timestamp:** \(result.environment.timestamp)
        **Device:** \(result.environment.deviceName) â€¢ \(result.environment.systemName) \
        \(result.environment.systemVersion)
        **Locale:** \(result.environment.localeIdentifier)

        ## Metrics
        - Duration: \(String(format: "%.2fs", result.metrics.duration))
        - Time to First Token: \(formattedInterval(result.metrics.timeToFirstToken))
        - Prompt Tokens (est.): \(result.metrics.promptTokenEstimate)
        - Response Tokens (est.): \(result.metrics.responseTokenEstimate)
        - Total Tokens (est.): \(result.metrics.totalTokenEstimate)
        - Tokens / sec: \(formattedTPS(result.metrics.tokensPerSecond))

        ## Response
        \(result.responseText)
        """
    }

    private func formattedInterval(_ interval: TimeInterval?) -> String {
        guard let interval else { return "n/a" }
        return String(format: "%.2fs", interval)
    }

    private func formattedTPS(_ value: Double?) -> String {
        guard let value else { return "n/a" }
        return String(format: "%.2f", value)
    }
}
